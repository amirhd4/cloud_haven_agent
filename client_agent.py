import os
import requests
import socket
import platform
import configparser
from pathlib import Path
from datetime import datetime
import subprocess
import gzip
import shutil
import time
import argparse
import asyncio
import websockets
import json
from apscheduler.schedulers.asyncio import AsyncIOScheduler

# ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø§ÛŒÙ† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¯Ø± Ú©Ù†Ø§Ø± agent ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯
from utils.security import generate_key, encrypt_file, decrypt_file
from drivers.postgres_driver import PostgresDriver
from drivers.mysql_driver import MySQLDriver

# ==============================================================================
#  Ø¨Ø®Ø´ Ú©Ø§Ù†ÙÛŒÚ¯ Ø¬Ø§Ø¨â€ŒÙ‡Ø§ - Ù…Ù†Ø¨Ø¹ Ø­Ù‚ÛŒÙ‚Øª Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ØªØµØ§Ù„ Ø§Ú©Ù†ÙˆÙ† Ø§ÛŒÙ†Ø¬Ø§Ø³Øª
# ==============================================================================
JOBS = {
    "pg_main": {
        "type": "postgresql",
        "bucket": "pg-main-backups",
        "config": {
            "host": "localhost", "port": 5432, "dbname": "online_shop",
            "user": "postgres", "password": "12345678"
        }
    },
    "mysql_web": {
        "type": "mysql",
        "bucket": "mysql-web-backups",
        "config": {
            "host": "localhost", "port": 3306, "database": "your_mysql_db",
            "user": "your_mysql_user", "password": "your_mysql_password"
        }
    }
}


# ==============================================================================

class ClientAgent:
    def __init__(self, server_url: str, jobs_config: dict):
        self.server_url = server_url
        self.jobs_config = jobs_config
        self.config_path = Path("client_config.ini")
        self.config = configparser.ConfigParser()
        self.access_token = None
        self.encryption_key = None
        self.paths_config = {}
        self.temp_dir = Path("./temp_backups")
        self.temp_dir.mkdir(exist_ok=True)
        self._load_config()
        self.no_proxy = {
            "http": None,
            "https": None,
        }

    def _load_config(self):
        if self.config_path.exists():
            self.config.read(self.config_path)
            if 'Auth' in self.config and 'AccessToken' in self.config['Auth']:
                self.access_token = self.config['Auth']['AccessToken']
            if 'Security' in self.config and 'EncryptionKey' in self.config['Security']:
                self.encryption_key = self.config['Security']['EncryptionKey'].encode()
            if 'Paths' in self.config:
                self.paths_config = dict(self.config['Paths'])

    def _save_config(self):
        with open(self.config_path, 'w') as f:
            self.config.write(f)

    def save_encryption_key(self, key: bytes):
        if 'Security' not in self.config: self.config['Security'] = {}
        self.config['Security']['EncryptionKey'] = key.decode()
        self._save_config()
        self.encryption_key = key
        print("âœ… Ú©Ù„ÛŒØ¯ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÙˆÙ„ÛŒØ¯ Ùˆ Ø¯Ø± 'client_config.ini' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

    def get_headers(self) -> dict:
        if not self.access_token: raise ValueError("Token not found.")
        return {"Authorization": f"Bearer {self.access_token}"}

    def _get_driver(self, job_config):
        db_type = job_config.get("type")
        bin_path = None
        if db_type == "postgresql":
            bin_path = self.paths_config.get("postgres_bin_path")
            return PostgresDriver(job_config["config"], self.temp_dir, bin_path)
        elif db_type == "mysql":
            bin_path = self.paths_config.get("mysql_bin_path")
            return MySQLDriver(job_config["config"], self.temp_dir, bin_path)
        else:
            raise ValueError(f"Ø¯Ø±Ø§ÛŒÙˆØ± Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù†ÙˆØ¹ '{db_type}' Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.")

    def upload_backup(self, file_path: Path, bucket_name: str):
        print(f"ğŸ“¤ [{datetime.now()}] Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ '{file_path.name}'...")
        upload_url = f"{self.server_url}/api/v1/storage/upload/{bucket_name}/{file_path.name}"
        try:
            with open(file_path, 'rb') as f:
                data = f.read()
            headers = self.get_headers()
            headers['Content-Type'] = 'application/octet-stream'
            response = requests.put(upload_url, data=data, headers=headers, timeout=300, proxies=self.no_proxy)
            response.raise_for_status()
            print(f"ğŸ‰ [{datetime.now()}] Ø¢Ù¾Ù„ÙˆØ¯ Ù…ÙˆÙÙ‚!")
            return True
        except Exception as e:
            raise RuntimeError(f"Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: {e}")

    def download_backup(self, object_name: str, bucket_name: str, destination_path: Path):
        print(f"ğŸ“¥ [{datetime.now()}] Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ '{object_name}'...")
        download_url = f"{self.server_url}/api/v1/storage/download/{bucket_name}/{object_name}"
        try:
            response = requests.get(download_url, headers=self.get_headers(), timeout=300, stream=True, proxies=self.no_proxy)
            response.raise_for_status()
            with open(destination_path, 'wb') as f:
                shutil.copyfileobj(response.raw, f)
            print("âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
            return True
        except Exception as e:
            raise RuntimeError(f"Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: {e}")

    def list_backups(self, bucket_name: str) -> list:
        print(f"ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ø¨Ú©Ø§Ù¾â€ŒÙ‡Ø§ Ø§Ø² Ø¨Ø§Ú©Øª '{bucket_name}'...")
        list_url = f"{self.server_url}/api/v1/storage/list/{bucket_name}"
        try:
            response = requests.get(list_url, headers=self.get_headers(), timeout=60, proxies=self.no_proxy)
            response.raise_for_status()
            files = response.json().get("files", [])
            print(f"âœ… {len(files)} ÙØ§ÛŒÙ„ Ø¨Ú©Ø§Ù¾ ÛŒØ§ÙØª Ø´Ø¯.")
            return files
        except Exception as e:
            raise RuntimeError(f"Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ø¨Ú©Ø§Ù¾â€ŒÙ‡Ø§ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: {e}")

    def run_backup_job(self, job_config: dict):
        db_name = job_config['config'].get('dbname') or job_config['config'].get('database')
        print(f"--- Ø´Ø±ÙˆØ¹ Ú†Ø±Ø®Ù‡ Ø§Ù…Ù† Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ '{db_name}' ---")
        if not self.encryption_key:
            raise RuntimeError("Ú©Ù„ÛŒØ¯ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ Ø¯Ø³ØªÙˆØ± 'generate-key' ÛŒÚ© Ú©Ù„ÛŒØ¯ Ø¨Ø³Ø§Ø²ÛŒØ¯.")
        compressed_path, encrypted_path = None, None
        try:
            driver = self._get_driver(job_config)
            compressed_path = driver.backup()
            encrypted_path = compressed_path.with_suffix(compressed_path.suffix + '.enc')
            print(f"ğŸ”’ Ø¯Ø± Ø­Ø§Ù„ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ Ø¨Ú©Ø§Ù¾...")
            encrypt_file(self.encryption_key, compressed_path, encrypted_path)
            print("âœ… Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
            if encrypted_path and encrypted_path.exists():
                self.upload_backup(encrypted_path, job_config["bucket"])
        except Exception as e:
            print(f"ğŸ”¥ ÛŒÚ© Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ú†Ø±Ø®Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø±Ø® Ø¯Ø§Ø¯: {e}")
        finally:
            if compressed_path and compressed_path.exists(): compressed_path.unlink()
            if encrypted_path and encrypted_path.exists(): encrypted_path.unlink()
            print("ğŸ—‘ï¸ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª Ù¾Ø§Ú© Ø´Ø¯Ù†Ø¯.")
        print("--- Ù¾Ø§ÛŒØ§Ù† Ú†Ø±Ø®Ù‡ Ø§Ù…Ù† Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ ---")

    def run_restore_job(self, job_config: dict, object_name: str):
        if not object_name.endswith('.enc'):
            raise ValueError("ÙØ§ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ ÛŒÚ© ÙØ§ÛŒÙ„ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ (Ø¨Ø§ Ù¾Ø³ÙˆÙ†Ø¯ .enc) Ù†ÛŒØ³Øª.")
        if not self.encryption_key:
            raise RuntimeError("Ú©Ù„ÛŒØ¯ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø§Ù…Ú©Ø§Ù† Ø±Ù…Ø²Ú¯Ø´Ø§ÛŒÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
        db_name = job_config['config'].get('dbname') or job_config['config'].get('database')
        print(f"--- Ø´Ø±ÙˆØ¹ Ú†Ø±Ø®Ù‡ Ø§Ù…Ù† Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø±Ø§ÛŒ '{db_name}' ---")
        encrypted_path = self.temp_dir / object_name
        compressed_path = self.temp_dir / object_name.removesuffix('.enc')
        raw_path = self.temp_dir / compressed_path.name.removesuffix('.gz')
        try:
            self.download_backup(object_name, job_config['bucket'], encrypted_path)
            print(f"ğŸ”‘ Ø¯Ø± Ø­Ø§Ù„ Ø±Ù…Ø²Ú¯Ø´Ø§ÛŒÛŒ ÙØ§ÛŒÙ„ '{encrypted_path.name}'...")
            decrypt_file(self.encryption_key, encrypted_path, compressed_path)
            print("âœ… Ø±Ù…Ø²Ú¯Ø´Ø§ÛŒÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
            print(f"ğŸ“¦ Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§ÛŒÙ„ '{compressed_path.name}'...")
            with gzip.open(compressed_path, 'rb') as f_in, open(raw_path, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
            driver = self._get_driver(job_config)
            driver.restore(raw_path)
        except Exception as e:
            print(f"ğŸ”¥ ÛŒÚ© Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ú†Ø±Ø®Ù‡ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø±Ø® Ø¯Ø§Ø¯: {e}")
        finally:
            if encrypted_path.exists(): encrypted_path.unlink()
            if compressed_path.exists(): compressed_path.unlink()
            if raw_path.exists(): raw_path.unlink()
            print("ğŸ—‘ï¸ ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù¾Ø§Ú© Ø´Ø¯Ù†Ø¯.")
        print("--- Ù¾Ø§ÛŒØ§Ù† Ú†Ø±Ø®Ù‡ Ø§Ù…Ù† Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ ---")

    async def _fetch_and_apply_schedules(self, scheduler: AsyncIOScheduler):
        """Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø² API Ø§Ø®ØªØµØ§ØµÛŒ Agent Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ø¯Ø± Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ Ù…Ø­Ù„ÛŒ Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        print(f"[{datetime.now()}] Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø² Ø³Ø±ÙˆØ±...")
        try:
            response = requests.get(f"{self.server_url}/api/v1/agent/my-schedules", headers=self.get_headers(), proxies=self.no_proxy)
            response.raise_for_status()
            schedules = response.json()

            scheduler.remove_all_jobs()
            print(f"[{datetime.now()}] Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§...")
            for schedule_data in schedules:
                if not schedule_data['is_active']: continue
                job_name = schedule_data['job_name']
                if job_name in self.jobs_config:
                    job_config = self.jobs_config[job_name]
                    print(f"  + Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ ÙˆØ¸ÛŒÙÙ‡ '{job_name}' Ø¨Ø§ cron: '{schedule_data['cron_string']}'")
                    scheduler.add_job(
                        self.run_backup_job, 'cron',
                        **self._parse_cron(schedule_data['cron_string']),
                        kwargs={"job_config": job_config}
                    )
                else:
                    print(f"  - Ù‡Ø´Ø¯Ø§Ø±: Ø¬Ø§Ø¨ Ø¨Ø§ Ù†Ø§Ù… '{job_name}' Ø¯Ø± Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø­Ù„ÛŒ (JOBS) ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§: {e}")

    @staticmethod
    def _parse_cron(cron_string: str) -> dict:
        parts = cron_string.split()
        if len(parts) != 5: return {}
        return {'minute': parts[0], 'hour': parts[1], 'day': parts[2], 'month': parts[3], 'day_of_week': parts[4]}

    async def _websocket_listener(self):
        scheduler = AsyncIOScheduler()
        await self._fetch_and_apply_schedules(scheduler)
        scheduler.start()
        print("\nâœ… Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ Ù…Ø­Ù„ÛŒ ÙØ¹Ø§Ù„ Ø´Ø¯.")

        ws_uri = f"ws://{self.server_url.split('//')[1]}/api/v1/clients/ws"
        headers = {"Authorization": f"Bearer {self.access_token}"}

        while True:
            try:
                async with websockets.connect(ws_uri, additional_headers=headers, proxy=None) as websocket:
                    print("âœ… Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ø³Ø±ÙˆØ± WebSocket Ù…ØªØµÙ„ Ø´Ø¯.")

                    # --- Ù…Ø¹Ø±ÙÛŒ Ø®ÙˆØ¯ Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ú©Ø§Ù…Ù„ (Ù†Ø§Ù… Ø¬Ø§Ø¨ -> Ù†Ø§Ù… Ø¨Ø§Ú©Øª) ---
                    jobs_map = {name: details['bucket'] for name, details in self.jobs_config.items()}
                    identify_payload = {
                        "type": "identify",
                        "jobs": jobs_map
                    }
                    await websocket.send(json.dumps(identify_payload))
                    print(f"â„¹ï¸ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ Ø¨Ù‡ Ø³Ø±ÙˆØ± Ù…Ø¹Ø±ÙÛŒ Ø´Ø¯: {jobs_map}")
                    print("...Ù…Ù†ØªØ¸Ø± Ø¯Ø±ÛŒØ§ÙØª ÙØ±Ù…Ø§Ù†...")

                    async for message in websocket:
                        print(f"\nğŸ“¨ ÙØ±Ù…Ø§Ù† Ø¬Ø¯ÛŒØ¯ Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯: {message}")
                        command = json.loads(message)
                        action = command.get("action")

                        if action == "reload_schedules":
                            print("ğŸ”„ Ø¯Ø±ÛŒØ§ÙØª ÙØ±Ù…Ø§Ù† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ...")
                            await self._fetch_and_apply_schedules(scheduler)
                            continue

                        job_name = command.get("job")
                        if not job_name or job_name not in self.jobs_config:
                            print(f"âŒ ÙØ±Ù…Ø§Ù† Ù†Ø§Ù…Ø¹ØªØ¨Ø±: job '{job_name}' Ø¯Ø± Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø­Ù„ÛŒ ØªØ¹Ø±ÛŒÙ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.")
                            continue

                        job_config = self.jobs_config[job_name]
                        if action == "backup":
                            self.run_backup_job(job_config)
                        elif action == "restore":
                            file_name = command.get("file")
                            if not file_name:
                                print("âŒ ÙØ±Ù…Ø§Ù† Ù†Ø§Ù…Ø¹ØªØ¨Ø±: Ø¨Ø±Ø§ÛŒ restore Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø§Ù„Ø²Ø§Ù…ÛŒ Ø§Ø³Øª.")
                                continue
                            self.run_restore_job(job_config, file_name)
            except Exception as e:
                print(f"ğŸ”¥ Ø®Ø·Ø§ÛŒ WebSocket: {e}. ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§ØªØµØ§Ù„ Ù…Ø¬Ø¯Ø¯ ØªØ§ 10 Ø«Ø§Ù†ÛŒÙ‡ Ø¯ÛŒÚ¯Ø±...")
                await asyncio.sleep(10)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Ú©Ù„Ø§ÛŒÙ†Øª Ø§Ù…Ù† Ø³ÛŒØ³ØªÙ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ")
    subparsers = parser.add_subparsers(dest='action', required=True)

    parser_keygen = subparsers.add_parser('generate-key', help="ÛŒÚ© Ú©Ù„ÛŒØ¯ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ø¬Ø¯ÛŒØ¯ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.")
    parser_listen = subparsers.add_parser('listen', help="Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ø³Ø±ÙˆÛŒØ³ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡ Ùˆ Ù…Ù†ØªØ¸Ø± Ø¯Ø³ØªÙˆØ±Ø§Øª Ø§Ø² Ø³Ø±ÙˆØ± Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯.")

    parser_backup = subparsers.add_parser('run-backup', help="ÛŒÚ© ÙˆØ¸ÛŒÙÙ‡ Ø¨Ú©Ø§Ù¾ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÛŒ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.")
    parser_backup.add_argument('--job', choices=JOBS.keys(), required=True, help="Ù†Ø§Ù… ÙˆØ¸ÛŒÙÙ‡â€ŒØ§ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯")

    parser_list = subparsers.add_parser('run-list', help="Ù„ÛŒØ³Øª Ø¨Ú©Ø§Ù¾â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ø¬Ø§Ø¨ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯.")
    parser_list.add_argument('--job', choices=JOBS.keys(), required=True)

    parser_restore = subparsers.add_parser('run-restore', help="ÛŒÚ© Ø¨Ú©Ø§Ù¾ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.")
    parser_restore.add_argument('--job', choices=JOBS.keys(), required=True)
    parser_restore.add_argument('--file', required=True)

    args = parser.parse_args()

    agent = ClientAgent(server_url="http://127.0.0.1:8000", jobs_config=JOBS)

    try:
        if args.action == 'generate-key':
            key = generate_key()
            agent.save_encryption_key(key)
        else:
            if not agent.access_token:
                raise ValueError(
                    "ØªÙˆÚ©Ù† Ø¯Ø³ØªØ±Ø³ÛŒ Ø¯Ø± client_config.ini ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ø§Ø¯Ù…ÛŒÙ†ØŒ ÛŒÚ© Ú©Ù„Ø§ÛŒÙ†Øª Ø¨Ø³Ø§Ø²ÛŒØ¯ Ùˆ ØªÙˆÚ©Ù† Ø¢Ù† Ø±Ø§ Ø¯Ø± Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯.")

            if args.action == 'listen':
                if not agent.encryption_key: raise ValueError(
                    "Ú©Ù„ÛŒØ¯ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø± client_config.ini ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ø¯Ø³ØªÙˆØ± 'generate-key' Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")
                asyncio.run(agent._websocket_listener())
            else:
                job_config = agent.jobs_config.get(args.job)
                # Note: job_config is guaranteed to exist due to 'choices=JOBS.keys()' in argparse

                if args.action == 'run-backup':
                    if not agent.encryption_key: raise ValueError("Ú©Ù„ÛŒØ¯ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ú©Ø§Ù¾ Ø§Ù„Ø²Ø§Ù…ÛŒ Ø§Ø³Øª.")
                    agent.run_backup_job(job_config)
                elif args.action == 'run-list':
                    backups = agent.list_backups(job_config['bucket'])
                    if backups:
                        print(f"\n--- Ù„ÛŒØ³Øª Ø¨Ú©Ø§Ù¾â€ŒÙ‡Ø§ Ø¯Ø± Ø¨Ø§Ú©Øª '{job_config['bucket']}' ---")
                        for f in backups: print(f"  - {f}")
                elif args.action == 'run-restore':
                    if not agent.encryption_key: raise ValueError("Ú©Ù„ÛŒØ¯ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ù„Ø²Ø§Ù…ÛŒ Ø§Ø³Øª.")
                    agent.run_restore_job(job_config, args.file)

    except (ValueError, RuntimeError, FileNotFoundError) as e:
        print(f"\nğŸ”´ ÛŒÚ© Ø®Ø·Ø§ÛŒ Ø¹Ù…Ù„ÛŒØ§ØªÛŒ Ø±Ø® Ø¯Ø§Ø¯: {e}")
    except Exception as e:
        print(f"\nğŸ”´ ÛŒÚ© Ø®Ø·Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ø´Ø¯Ù‡ Ø±Ø® Ø¯Ø§Ø¯: {e}")